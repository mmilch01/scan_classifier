{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import getpass, ipywidgets as ipw, os, json, shlex, io, re, tempfile, subprocess\n",
    "import numpy as np,csv,warnings,pickle,sys,tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from IPython.display import FileLink\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from juxnat_lib.xnat_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOF_Classifier:\n",
    "    def __init__(self):\n",
    "        self.classifier=[]\n",
    "        self.vectorizer=[]\n",
    "        self._class_vectorizer=None\n",
    "        #important: must be in aphpabetical order for vectorizer to work correctly.\n",
    "        self._classes=['CBF','CBV','DSC','DWI','FA','MD','MPRAGE','MTT','OT','PBP','SWI','T1hi','T1lo','T2FLAIR','T2hi','T2lo','TRACEW','TTP']\n",
    "        #self._scan_list=[]\n",
    "    def load_json(self, json_file):\n",
    "        with open(json_file, 'r') as fp:\n",
    "            out_dict=json.loads(fp.read())\n",
    "        return out_dict    \n",
    "    def save_json(self, var, file):\n",
    "        with open(file,'w') as fp:\n",
    "            json.dump(var, fp) \n",
    "    '''\n",
    "    Assign HOF ID's to scans using associative table look-up.\n",
    "    '''\n",
    "    def assign_hofids_slist(self,scans):\n",
    "        for s in scans:\n",
    "            descr=re.sub(' ','',s['series_description'])\n",
    "            cmd=\"slist qd \"+\"\\\"\" + descr + \"\\\"\"\n",
    "            try:\n",
    "                hof_id=os.popen(cmd).read().split()[1]\n",
    "            except:\n",
    "                hof_id=\"\"\n",
    "            #print(hof_id)\n",
    "            s['hof_id']=hof_id\n",
    "            #out.value=\"{}/{}\".format(s['series_description'],hof_id)\n",
    "        return scans\n",
    "    \n",
    "    def write_scans_csv(self, scans, file):\n",
    "        with open(file, 'w') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, scans[0].keys())\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(scans)\n",
    "            \n",
    "    def read_scans_csv(self, file):\n",
    "        with open(file,'r') as inf:\n",
    "            reader = csv.DictReader(inf)\n",
    "            scans=[{k: str(v) for k,v in row.items()} \n",
    "                      for row in csv.DictReader(inf,skipinitialspace=True)]\n",
    "        return scans    \n",
    "       \n",
    "    '''\n",
    "    Create vocabulary from the bag of words. These will act as features.\n",
    "    '''\n",
    "    def gen_vocabulary(self,scans):\n",
    "        descs=self.prepare_descs(scans)\n",
    "        vectorizer=CountVectorizer(min_df=0)\n",
    "        vectorizer.fit(descs)\n",
    "        self.vectorizer=vectorizer\n",
    "        print('the length of vocabulary is ',len(vectorizer.vocabulary_))\n",
    "     \n",
    "    #for logreg/svm output, categorical labels are stored as strings\n",
    "    def prepare_training_vectors(self,scans):\n",
    "        #labels vector.\n",
    "        vectorized_descs=self.gen_bow_vectors(scans)\n",
    "        y=[ s['hof_id'] for s in scans ]\n",
    "        return vectorized_descs,y\n",
    "    \n",
    "    #for a NN output, categorical labels are stored as BOW over vocabulary of class labels.\n",
    "    def prepare_training_vectors_nn(self,scans,gen_hofids=True):\n",
    "        if self._class_vectorizer is None:\n",
    "            vectorizer=CountVectorizer(min_df=0)\n",
    "            vectorizer.fit(self._classes)\n",
    "            self._class_vectorizer=vectorizer\n",
    "        vectorizer=self._class_vectorizer\n",
    "        vectorized_descs=self.gen_bow_vectors(scans)\n",
    "        hofids=[ s['hof_id'] for s in scans ] if gen_hofids else []        \n",
    "        return vectorized_descs,vectorizer.transform(hofids).toarray()\n",
    "    \n",
    "    def prepare_descs(self,scans):\n",
    "        #descs are 'sentences' that contain series description and log-compressed number of frames.\n",
    "        descs=[]\n",
    "        for s in scans:\n",
    "            desc=(re.sub('[^0-9a-zA-Z ]+',' ',s['series_description'])).split()\n",
    "            #compressed representation of the number of frames.\n",
    "            try:\n",
    "                frames='frames{}'.format(str(int(np.around(np.log(1.0+float(s['frames']))*3.0))))\n",
    "            except:\n",
    "                frames='frames0'\n",
    "            desc.append(frames)\n",
    "            descs.append(' '.join([s for s in desc if ((not s.isdigit()) and (len(s)>1)) ]))\n",
    "        return descs\n",
    "        \n",
    "    def gen_bow_vectors(self,scans):\n",
    "        if not self.vectorizer: return []\n",
    "        descs=self.prepare_descs(scans)\n",
    "        return self.vectorizer.transform(descs).toarray()    \n",
    "    \n",
    "    def train_nn(self,X,y,test_split,epochs=10,batch_size=10):\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_split,random_state=1000)\n",
    "        input_dim=X_train.shape[1]\n",
    "        print('input_dim:',input_dim)\n",
    "        model = Sequential()\n",
    "        model.add(layers.Dense(36,input_dim=input_dim,activation='relu'))\n",
    "        #model.add(layers.Dense(18,activation='relu'))\n",
    "        model.add(layers.Dense(len(self._classes),activation='sigmoid'))\n",
    "        print('output_dim:',len(self._classes))\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
    "        model.summary()\n",
    "        self.classifier=model\n",
    "        #self.classifier.fit(X_train,y_train,epochs=10,verbose=True,validation_data=(X_test,y_test),batch_size=10)\n",
    "        hist=self.classifier.fit(X_train,y_train,epochs=epochs,verbose=True,validation_data=(X_test,y_test),batch_size=batch_size)\n",
    "        self.plot_nn_train_history(hist)\n",
    "        \n",
    "    def plot_nn_train_history(self,history):\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        x = range(1, len(acc) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(x, acc, 'b', label='Training acc')\n",
    "        plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(x, loss, 'b', label='Training loss')\n",
    "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "        \n",
    "    def infer_nn(self,scans):\n",
    "        vecs,ids=self.prepare_training_vectors_nn(scans,False)\n",
    "        y_fit=self.classifier.predict(vecs)\n",
    "        hofids=[ self._classes[np.argmax(y_fit[i])] for i in range(len(y_fit)) ]\n",
    "        return hofids\n",
    "        \n",
    "    def train_classifier(self,X,y,test_split):\n",
    "        descs_train,descs_test,y_train,y_test=train_test_split(X,y,test_size=test_split,random_state=1000)\n",
    "        #classifier=LogisticRegression()\n",
    "        classifier=LinearSVC()\n",
    "        #classifier=SVC()\n",
    "        classifier.fit(descs_train,y_train)\n",
    "        scoreTest=classifier.score(descs_test,y_test)\n",
    "        scoreTrain=classifier.score(descs_train,y_train)\n",
    "        print('Test accuracy:', scoreTest, \" train accuracy:\",scoreTrain)        \n",
    "        self.classifier=classifier\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    def _merge_hofids(self,scans,hofids):\n",
    "        for s in scans:\n",
    "            descr=re.sub(' ','',s['series_description'])\n",
    "            cmd=\"slist qd \"+\"\\\"\" + descr + \"\\\"\"\n",
    "            try:\n",
    "                hof_id=os.popen(cmd).read().split()[1]\n",
    "            except:\n",
    "                hof_id=\"\"\n",
    "            #print(hof_id)\n",
    "            s['hof_id']=hof_id\n",
    "            out.value=\"{}/{}\".format(s['series_description'],hof_id)\n",
    "        \n",
    "    def _predict_classifier(self,X):\n",
    "        if not self.classifier: return []\n",
    "        return self.classifier.predict(X)\n",
    "        \n",
    "    def predict_classifier(self, scans):\n",
    "        vectorized_descs=self.gen_bow_vectors(scans)\n",
    "        labels=self._predict_classifier(vectorized_descs)\n",
    "        for i,s in enumerate(scans):\n",
    "            s['hof_id']=labels[i]\n",
    "        return scans\n",
    "    \n",
    "    def is_valid_model(self):\n",
    "        return (self.vectorizer and self.classifier)    \n",
    "        \n",
    "    def save_model_nn(self,rt):\n",
    "        pickle.dump(self.vectorizer,open(rt+'.vec','wb'))\n",
    "        self.classifier.save(rt+'.hd5')\n",
    "        \n",
    "    def load_model_nn(self,rt):\n",
    "        self.vectorizer=pickle.load(open(rt+'.vec','rb'))\n",
    "        self.classifier=tf.keras.models.load_model(rt+'.hd5')\n",
    "    \n",
    "    def save_model(self, file):\n",
    "        pickle.dump([self.vectorizer,self.classifier],open(file,'wb'))\n",
    "                    \n",
    "    def load_model(self, file):\n",
    "        self.vectorizer,self.classifier=pickle.load(open(file,'rb'))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanClassifierLauncher(FrontDesk):\n",
    "    def __init__(self,sp):\n",
    "        self._xi=XnatIterator(sp)        \n",
    "        btn_lay={'width':'200pt'}\n",
    "        self._fupl=ipw.FileUpload(accept='.csv',multiple=False)\n",
    "        self._fupl.observe(self.read_uploaded_file)\n",
    "        self._fupl_label=ipw.Label(value='Status: waiting to upload csv',layout=btn_lay)\n",
    "        self._fupl_drop=ipw.Dropdown(options=['experiments','scans-raw','scans-classified'],\n",
    "                                     value='scans-raw',description='csv contents',disabled=False, layout=btn_lay)\n",
    "        self._fupl_box=ipw.HBox([self._fupl_drop, self._fupl, self._fupl_label])\n",
    "                \n",
    "        self._coll_btn=ipw.Button(description='Collect scans',layout=btn_lay)\n",
    "        self._coll_btn.on_click(self.collect_scans)\n",
    "        self._coll_status=ipw.Label(value='Status: ready to run.')\n",
    "        self._coll_box=ipw.HBox([self._coll_btn,self._coll_status])\n",
    "        \n",
    "        self._out_lnk=ipw.Output()\n",
    "        \n",
    "        self._classify_btn=ipw.Button(description='Classify scans',layout=btn_lay)\n",
    "        self._classify_btn.on_click(self.run_classifier)\n",
    "        self._classify_lbl=ipw.Label(value='Status: ready')\n",
    "        self._classify_lnk=ipw.Output()\n",
    "        self._classify_box=ipw.VBox([ipw.HBox([self._classify_btn,self._classify_lbl]),self._classify_lnk])\n",
    "\n",
    "        self._group_btn=ipw.Button(description='Group scans by experiment',layout=btn_lay)\n",
    "        self._group_btn.on_click(self.group_scans)\n",
    "        self._group_lbl=ipw.Label(value='Status: ready')\n",
    "        self._group_lnk=ipw.Output()\n",
    "        self._group_box=ipw.VBox([ipw.HBox([self._group_btn,self._group_lbl]),self._group_lnk])\n",
    "\n",
    "        self._detect_gad_btn=ipw.Button(description='Detect GAD in T1hi')\n",
    "        self._detect_gad_btn.on_click(self.run_detect_gad)\n",
    "                        \n",
    "        self.main_box=ipw.VBox([self._fupl_box,self._coll_box,self._out_lnk,self._classify_box,self._group_box])\n",
    "        \n",
    "        hc=HOF_Classifier()\n",
    "        hc.load_model_nn('./scan_classifier_nn.11.26.2019')\n",
    "        self.hof_classifier=hc\n",
    "        \n",
    "    def write_scans_csv(self, scans, file):\n",
    "        with open(file, 'w') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, scans[0].keys())\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(scans)\n",
    "            \n",
    "    def read_uploaded_file(self,b):\n",
    "        fupl=self._fupl\n",
    "        if not bool(fupl): return False\n",
    "        keys=list(fupl.value)        \n",
    "        print(keys)\n",
    "        try:\n",
    "            csv_reader = csv.DictReader(io.TextIOWrapper(io.BytesIO(fupl.value[keys[0]]['content'])),skipinitialspace=True)\n",
    "            self._rows=[{k: str(v) for k,v in row.items()} for row in csv_reader]\n",
    "        except:\n",
    "            self._fupl_label.value='Status: cannot parse csv'\n",
    "            return False\n",
    "        \n",
    "        self._fupl_label.value='Status: csv loaded with {} rows'.format(len(self._rows))\n",
    "        if self._fupl_drop.value=='scans-raw' or self._fupl_drop.value=='scans-classified':\n",
    "            self.scans=self._rows\n",
    "        #print(self._exps)\n",
    "        return True\n",
    "    \n",
    "    def refresh(self):\n",
    "        self.enable_nav_prev(False)\n",
    "    \n",
    "    def show_file_link(self,out,file):\n",
    "        out.outputs=();  f=FileLink(file)\n",
    "        with out:\n",
    "            display(f)\n",
    "            \n",
    "    def group_scans(self,b):\n",
    "        def add_val(dic,key,val):\n",
    "            if key in dic: dic[key]+=[val]\n",
    "            else: dic[key]=[val]\n",
    "                \n",
    "        def add_default_vals(dic,keys):\n",
    "            for key in keys:\n",
    "                if key not in dic: dic[key]=[]\n",
    "                    \n",
    "        scans=self.scans\n",
    "        d={} #experiments\n",
    "        def_keys=['T1c','T1nc','T2','T2FLAIR']\n",
    "        def_keys_imp=['T1c_imp','T1nc_imp','T2_imp','T2FLAIR_imp']\n",
    "        \n",
    "        confs={} #runtime configurations, keyed by subject. \n",
    "        for r in self.scans:\n",
    "            frames,sid,subj,exp,hofid,gad=r['frames'],r['ID'],r['subject'],r['experiment'],r['hofid'],r['gad']  \n",
    "            if exp in d: expd=d[exp]\n",
    "            else: d[exp]={}; expd=d[exp]; add_default_vals(expd,def_keys)\n",
    "            nt1c=expd['nT1c'] if 'nT1c' in expd else 0\n",
    "            expd['subject']=subj\n",
    "            if gad==\"1\": add_val(expd,'T1c',sid); nt1c+=1\n",
    "            elif hofid=='MPRAGE' or hofid=='T1hi': add_val(expd,'T1nc',sid)\n",
    "            elif hofid=='T2hi': add_val(expd,'T2',sid)\n",
    "            elif hofid=='T2FLAIR': add_val(expd,'T2FLAIR',sid)\n",
    "            expd['nT1c']=nt1c\n",
    "            \n",
    "            if subj in confs: conf=confs[subj]\n",
    "            else: \n",
    "                confs[subj]={}; conf=confs[subj]; conf['experiments']=[exp]; \n",
    "                conf['subject']=subj; conf['targ_experiment']=None\n",
    "            if exp not in conf['experiments']: conf['experiments']+=[exp]\n",
    "               \n",
    "            if nt1c>0:\n",
    "                if 'nT1c' not in conf: conf['nT1c']=nt1c; conf['targ_experiment']=exp\n",
    "                elif conf['nT1c']<nt1c: conf['nT1c']=nt1c; conf['targ_experiment']=exp\n",
    "            else:\n",
    "                conf['nT1c']=0\n",
    "                           \n",
    "        #l1=list(d.values()); print(l1[:5]) \n",
    "        #l1=list(confs.values()); print(l1[:5])\n",
    "        \n",
    "        #now create run configurations, one configuration per subject.\n",
    "        for conf_key in confs.keys():\n",
    "            conf=confs[conf_key]\n",
    "            add_default_vals(conf,def_keys)\n",
    "            add_default_vals(conf,def_keys_imp)\n",
    "            texp_name=conf['targ_experiment']\n",
    "            if texp_name is not None:\n",
    "                texp=d[texp_name]\n",
    "                for k in def_keys: conf[k]=';'.join(texp[k]) if len(texp[k])>0 else 'NA'\n",
    "                for k in def_keys_imp: conf[k]='NA'\n",
    "            else: texp=None\n",
    "            for exp in conf['experiments']: \n",
    "                if exp==texp_name: continue\n",
    "                exp_dic=d[exp]\n",
    "                for i in range(len(def_keys_imp)):\n",
    "                    k,k1=def_keys[i],def_keys_imp[i]\n",
    "                    l=[ exp+\":\"+kv for kv in exp_dic[k] if len(kv)>0 ]                                        \n",
    "                    conf[k1]=';'.join(l) if len(l)>0 else 'NA'\n",
    "                    \n",
    "                '''\n",
    "                for k in def_keys:\n",
    "                    l=[ exp+\":\"+kv for kv in exp_dic[k] ]\n",
    "                    l1=[conf[k]] if len(conf[k])>0 else []\n",
    "                    l1=l1+l if len(l)>0 else l1\n",
    "                    conf[k]='' if len(l1)<1 else ','.join(l1)\n",
    "                '''\n",
    "            #print('conf:',conf)\n",
    "\n",
    "        '''                    \n",
    "        exps=[]\n",
    "        for key in d.keys():\n",
    "            e=d[key]; e['experiment']=key\n",
    "            add_default_vals(e,def_keys)\n",
    "            exps+=[e]\n",
    "            \n",
    "        '''\n",
    "        #print(exps[:20])\n",
    "        fil='run_configurations.csv'\n",
    "        vals=list(confs.values())\n",
    "        #print(vals[:5])\n",
    "        self.write_scans_csv(list(confs.values()),fil)\n",
    "        self.show_file_link(self._group_lnk,fil)        \n",
    "    \n",
    "    def collect_scans(self,b):\n",
    "        subjs=[ s['Subject'] for s in self._rows ]\n",
    "        #print(\"len_subj\",len(subjs))\n",
    "        exps=[ s['Experiment'] for s in self._rows ]\n",
    "        #print(\"len_exps\", len(exps))\n",
    "        \n",
    "        self.scans=self._xi.list_scans_in_experiments(subjs,exps,self._coll_status)\n",
    "        self._coll_status='Status: found {} scans'.format(len(self.scans))\n",
    "        fil='all_scans.csv'\n",
    "        self.write_scans_csv(self.scans,fil)\n",
    "        self.show_file_link(self._out_lnk,fil)\n",
    "            \n",
    "    def run_classifier(self,b):\n",
    "        hofids=self.hof_classifier.infer_nn(self.scans)\n",
    "        for i in range(len(self.scans)):\n",
    "            self.scans[i]['hofid']=hofids[i]\n",
    "        fil='all_scans_hofid.csv'\n",
    "        self.write_scans_csv(self.scans,fil)\n",
    "        self.show_file_link(self._classify_lnk,fil)\n",
    "        \n",
    "    def run_detect_gad(self,b):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e45a5ba44148ac9e07303eacf50c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4>Login</h4>'), VBox(children=(VBox(children=(HBox(children=(Text(value='', descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ser_file=\"./scan_classifier_gui_params.json\"\n",
    "xl=XNATLogin(ser_file)\n",
    "#xl.sp.project='IMIND_PILOT'\n",
    "#xl.sp.project='CONDR_METS'\n",
    "#xl.sp.project='M19004_NSCLC_2'\n",
    "#xl.sp.project='CONDR'\n",
    "xl.sp.project='M19004_AS' #acoustic schwannoma\n",
    "\n",
    "#xl.sp.project='rsfMRITumor'\n",
    "\n",
    "#xl.sp.project='I3CR'\n",
    "#xl.sp.project='CLEAR_LUNGS' \n",
    "#xl.sp.project='NON_COV_PNEUM' \n",
    "\n",
    "scl=ScanClassifierLauncher(xl.sp)\n",
    "pages=[\n",
    "        {'title':'Login','frontdesk':xl,'plumbing':None,'prev_label':None,'next_label':'Configure & run'},\n",
    "        {'title':'Classify scans','frontdesk':scl,'plumbing':None,'prev_label':'Login','next_label':None}\n",
    "    ]\n",
    "g=GUIBook(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
