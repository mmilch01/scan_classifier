{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/NRG/mmilchenko/lib/python3-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shared/NRG/mmilchenko/lib/python3-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shared/NRG/mmilchenko/lib/python3-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shared/NRG/mmilchenko/lib/python3-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shared/NRG/mmilchenko/lib/python3-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shared/NRG/mmilchenko/lib/python3-tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import ipywidgets as ipw\n",
    "import os\n",
    "import json\n",
    "import shlex\n",
    "import io\n",
    "import re\n",
    "import tempfile\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from IPython.display import FileLink\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerParams:\n",
    "    '''\n",
    "    Container parameters received from XNAT\n",
    "    '''\n",
    "    def __init__(self,server=None, user=None, password=None, project=None,subject=None,experiment=None):\n",
    "        self.server,self.user,self.password,self.project,self.subject,self.experiment= \\\n",
    "            server,user,password,project,subject,experiment\n",
    "        self.jsession=''\n",
    "        self.connected=False\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"server:{}, user: {}, project: {}, subject: {}, experiment: {}, connected: {}\".\\\n",
    "            format(self.server,self.user,self.project,self.subject,self.experiment,self.connected)\n",
    "        \n",
    "    def connect(self):\n",
    "        cmd=\"curl -k -u \"+ self.user+\":\"+self.password+ \\\n",
    "            \" \"+self.server+\"/data/JSESSION\"        \n",
    "        self.jsession=os.popen(cmd).read()\n",
    "        self.connected=(len(self.jsession)==32)\n",
    "        return self.connected\n",
    "    \n",
    "class XnatIterator:\n",
    "    def __init__(self,sp):\n",
    "        self.sp=sp\n",
    "        self._subjects=[]\n",
    "        self._experiments=[]\n",
    "        self._scans=[]\n",
    "            \n",
    "    def _curl_cmd_prefix(self):\n",
    "        return \"curl  -k --cookie JSESSIONID=\" + self.sp.jsession\n",
    "    \n",
    "    def _curl_cmd_path(self,path):\n",
    "        return shlex.quote(self.sp.server+\"/data/archive/projects/\"+self.sp.project+path)\n",
    "    \n",
    "    def _curl_cmd(self,path):        \n",
    "        cmd=self._curl_cmd_prefix()+' '+self._curl_cmd_path(path)\n",
    "        out=os.popen(cmd).read()\n",
    "        return(out)\n",
    "        \n",
    "    def curl_download_single_file(self,path,dest):\n",
    "        cmd=self._curl_cmd_prefix()+' -o '+dest+' '+ self.sp.server + path\n",
    "        return os.popen(cmd).read()\n",
    "        \n",
    "    def set_project(self,pr):\n",
    "        self.sp.project=pr\n",
    "    \n",
    "    def list_subjects(self):\n",
    "        tq=self._curl_cmd('/subjects?format=json')\n",
    "        try: \n",
    "            df=json.loads(tq)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "        subjs=sorted(df['ResultSet']['Result'], key=lambda k:k['label'])        \n",
    "        self._subjects=[f['label'] for f in subjs]        \n",
    "        return self._subjects\n",
    "    \n",
    "    def scan_file_loader(self,scans,tdir,lock):\n",
    "        for s in scans:\n",
    "            #print(s)\n",
    "            files=self.list_scan_files(s['subject'],s['experiment'],s['ID'])\n",
    "            if len(files)>0:\n",
    "                t=tdir+'/'+s['subject']+'_'+s['experiment']+'_'+s['ID']\n",
    "                self.curl_download_single_file(files[0],t+'.dcm')\n",
    "                os.system(\"dcmj2pnm +G +Wn +on \"+t+\".dcm \"+ t + \".png\")\n",
    "                os.system( \"rm -f \" + t + \".dcm\" )\n",
    "                lock.acquire()\n",
    "                s['png'] = t+\".png\"\n",
    "                lock.release()\n",
    "            else:\n",
    "                s['png']='N/A'\n",
    "    \n",
    "    def list_experiments(self,subject):\n",
    "        tq=self._curl_cmd('/subjects/'+subject+\"/experiments?xsiType=xnat:imageSessionData&format=json\") \n",
    "        try:\n",
    "            df=json.loads(tq)\n",
    "        except:\n",
    "            print ('error listing experiments!')\n",
    "            return []\n",
    "        exps=sorted(df['ResultSet']['Result'], key=lambda k:k['date'])\n",
    "        self._experiments=[f['label'] for f in exps]\n",
    "        return self._experiments\n",
    "    \n",
    "    def list_scans(self,subject,experiment, listDcmFiles=False):\n",
    "        sf=self._curl_cmd('/subjects/'+ subject +'/experiments/' \\\n",
    "            +experiment + \"/scans?columns=ID,frames,type,series_description\")\n",
    "        try: \n",
    "            df=json.loads(sf)\n",
    "        except:\n",
    "            return []\n",
    "        self._scans=sorted(df['ResultSet']['Result'], key=lambda k:k['xnat_imagescandata_id'])\n",
    "        for s in self._scans:            \n",
    "            s['subject']=subject\n",
    "            s['experiment']=experiment\n",
    "        \n",
    "        if listDcmFiles:\n",
    "            for s in self._scans:\n",
    "                files=self.list_scan_files(subject,experiment,s['ID'])\n",
    "                s['files']=files\n",
    "        return self._scans\n",
    "    \n",
    "    def get_dcm_files_for_scans(self,subject,experiment,scans):\n",
    "        for s in scans:\n",
    "            files=self.list_scan_files(subject,experiment,s['ID'])\n",
    "            s['files']=files\n",
    "        \n",
    "    def list_scan_files(self,subject,experiment,scan):\n",
    "        sf=self._curl_cmd('/subjects/'+ subject +'/experiments/' \\\n",
    "            +experiment + '/scans/'+scan+'/resources/DICOM/files')\n",
    "        try: df=json.loads(sf)\n",
    "        except:\n",
    "            return []\n",
    "        lst=sorted(df['ResultSet']['Result'], key=lambda k:k['Name'])\n",
    "        return [ f['URI'] for f in lst ]\n",
    "    \"\"\"\n",
    "    list all scans in project, filtered by subject prefix. \n",
    "    Display progres in output textarea.\n",
    "    Save output in speficified json file.\n",
    "    \"\"\"\n",
    "    def list_scans_all(self,subjects,subject_prefix,output):\n",
    "        scans=[]\n",
    "        ns=0\n",
    "        for su in subjects:\n",
    "            if not su.lower().startswith(subject_prefix.lower()): continue\n",
    "            experiments=self.list_experiments(su)\n",
    "            for e in experiments:\n",
    "                if output: output.value='running, found {} scans'.format(ns)\n",
    "                sscans=self.list_scans(su,e)\n",
    "                for s in sscans:\n",
    "                    scans.append(s)\n",
    "                    ns+=1\n",
    "        return scans\n",
    "    \n",
    "class HOF_Classifier:\n",
    "    def __init__(self):\n",
    "        self.classifier=[]\n",
    "        self.vectorizer=[]\n",
    "        self._class_vectorizer=None\n",
    "        #important: must be in aphpabetical order for vectorizer to work correctly.\n",
    "        self._classes=['CBF','CBV','DSC','DWI','FA','MD','MPRAGE','MTT','OT','PBP','SWI','T1hi','T1lo','T2FLAIR','T2hi','T2lo','TRACEW','TTP']\n",
    "        #self._scan_list=[]\n",
    "    def load_json(self, json_file):\n",
    "        with open(json_file, 'r') as fp:\n",
    "            out_dict=json.loads(fp.read())\n",
    "        return out_dict    \n",
    "    def save_json(self, var, file):\n",
    "        with open(file,'w') as fp:\n",
    "            json.dump(var, fp) \n",
    "    '''\n",
    "    Assign HOF ID's to scans using associative table look-up.\n",
    "    '''\n",
    "    def assign_hofids_slist(self,scans):\n",
    "        for s in scans:\n",
    "            descr=re.sub(' ','',s['series_description'])\n",
    "            cmd=\"slist qd \"+\"\\\"\" + descr + \"\\\"\"\n",
    "            try:\n",
    "                hof_id=os.popen(cmd).read().split()[1]\n",
    "            except:\n",
    "                hof_id=\"\"\n",
    "            #print(hof_id)\n",
    "            s['hof_id']=hof_id\n",
    "            #out.value=\"{}/{}\".format(s['series_description'],hof_id)\n",
    "        return scans\n",
    "    \n",
    "    def write_scans_csv(self, scans, file):\n",
    "        with open(file, 'w') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, scans[0].keys())\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(scans)\n",
    "            \n",
    "    def read_scans_csv(self, file):\n",
    "        with open(file,'r') as inf:\n",
    "            reader = csv.DictReader(inf)\n",
    "            scans=[{k: str(v) for k,v in row.items()} \n",
    "                      for row in csv.DictReader(inf,skipinitialspace=True)]\n",
    "        return scans    \n",
    "       \n",
    "    '''\n",
    "    Create vocabulary from the bag of words. These will act as features.\n",
    "    '''\n",
    "    def gen_vocabulary(self,scans):\n",
    "        descs=self.prepare_descs(scans)\n",
    "        vectorizer=CountVectorizer(min_df=0)\n",
    "        vectorizer.fit(descs)\n",
    "        self.vectorizer=vectorizer\n",
    "        print('the length of vocabulary is ',len(vectorizer.vocabulary_))\n",
    "     \n",
    "    #for logreg/svm output, categorical labels are stored as strings\n",
    "    def prepare_training_vectors(self,scans):\n",
    "        #labels vector.\n",
    "        vectorized_descs=self.gen_bow_vectors(scans)\n",
    "        y=[ s['hof_id'] for s in scans ]\n",
    "        return vectorized_descs,y\n",
    "    \n",
    "    #for a NN output, categorical labels are stored as BOW over vocabulary of class labels.\n",
    "    def prepare_training_vectors_nn(self,scans,gen_hofids=True):\n",
    "        if self._class_vectorizer is None:\n",
    "            vectorizer=CountVectorizer(min_df=0)\n",
    "            vectorizer.fit(self._classes)\n",
    "            self._class_vectorizer=vectorizer\n",
    "        vectorizer=self._class_vectorizer\n",
    "        vectorized_descs=self.gen_bow_vectors(scans)\n",
    "        hofids=[ s['hof_id'] for s in scans ] if gen_hofids else []        \n",
    "        return vectorized_descs,vectorizer.transform(hofids).toarray()\n",
    "    \n",
    "    def prepare_descs(self,scans):\n",
    "        #descs are 'sentences' that contain series description and log-compressed number of frames.\n",
    "        descs=[]\n",
    "        for s in scans:\n",
    "            desc=(re.sub('[^0-9a-zA-Z ]+',' ',s['series_description'])).split()\n",
    "            #compressed representation of the number of frames.\n",
    "            try:\n",
    "                frames='frames{}'.format(str(int(np.around(np.log(1.0+float(s['frames']))*3.0))))\n",
    "            except:\n",
    "                frames='frames0'\n",
    "            desc.append(frames)\n",
    "            descs.append(' '.join([s for s in desc if ((not s.isdigit()) and (len(s)>1)) ]))\n",
    "        return descs\n",
    "        \n",
    "    def gen_bow_vectors(self,scans):\n",
    "        if not self.vectorizer: return []\n",
    "        descs=self.prepare_descs(scans)\n",
    "        return self.vectorizer.transform(descs).toarray()    \n",
    "    \n",
    "    def train_nn(self,X,y,test_split,epochs=10,batch_size=10):\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_split,random_state=1000)\n",
    "        input_dim=X_train.shape[1]\n",
    "        print('input_dim:',input_dim)\n",
    "        model = Sequential()\n",
    "        model.add(layers.Dense(36,input_dim=input_dim,activation='relu'))\n",
    "        #model.add(layers.Dense(18,activation='relu'))\n",
    "        model.add(layers.Dense(len(self._classes),activation='sigmoid'))\n",
    "        print('output_dim:',len(self._classes))\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
    "        model.summary()\n",
    "        self.classifier=model\n",
    "        #self.classifier.fit(X_train,y_train,epochs=10,verbose=True,validation_data=(X_test,y_test),batch_size=10)\n",
    "        hist=self.classifier.fit(X_train,y_train,epochs=epochs,verbose=True,validation_data=(X_test,y_test),batch_size=batch_size)\n",
    "        self.plot_nn_train_history(hist)\n",
    "        \n",
    "    def plot_nn_train_history(self,history):\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        x = range(1, len(acc) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(x, acc, 'b', label='Training acc')\n",
    "        plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(x, loss, 'b', label='Training loss')\n",
    "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "        \n",
    "    def infer_nn(self,scans):\n",
    "        vecs,ids=self.prepare_training_vectors_nn(scans,False)\n",
    "        y_fit=self.classifier.predict(vecs)        \n",
    "        hofids=[ self._classes[np.argmax(y_fit[i])] for i in range(len(y_fit)) ]\n",
    "        return hofids        \n",
    "        \n",
    "    def train_classifier(self,X,y,test_split):\n",
    "        descs_train,descs_test,y_train,y_test=train_test_split(X,y,test_size=test_split,random_state=1000)\n",
    "        #classifier=LogisticRegression()\n",
    "        classifier=LinearSVC()\n",
    "        #classifier=SVC()\n",
    "        classifier.fit(descs_train,y_train)\n",
    "        scoreTest=classifier.score(descs_test,y_test)\n",
    "        scoreTrain=classifier.score(descs_train,y_train)\n",
    "        print('Test accuracy:', scoreTest, \" train accuracy:\",scoreTrain)        \n",
    "        self.classifier=classifier\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    def _merge_hofids(self,scans,hofids):\n",
    "        for s in scans:\n",
    "            descr=re.sub(' ','',s['series_description'])\n",
    "            cmd=\"slist qd \"+\"\\\"\" + descr + \"\\\"\"\n",
    "            try:\n",
    "                hof_id=os.popen(cmd).read().split()[1]\n",
    "            except:\n",
    "                hof_id=\"\"\n",
    "            #print(hof_id)\n",
    "            s['hof_id']=hof_id\n",
    "            out.value=\"{}/{}\".format(s['series_description'],hof_id)\n",
    "        \n",
    "    def _predict_classifier(self,X):\n",
    "        if not self.classifier: return []\n",
    "        return self.classifier.predict(X)\n",
    "        \n",
    "    def predict_classifier(self, scans):\n",
    "        vectorized_descs=self.gen_bow_vectors(scans)\n",
    "        labels=self._predict_classifier(vectorized_descs)\n",
    "        for i,s in enumerate(scans):\n",
    "            s['hof_id']=labels[i]\n",
    "        return scans\n",
    "    \n",
    "    def is_valid_model(self):\n",
    "        return (self.vectorizer and self.classifier)    \n",
    "        \n",
    "    def save_model_nn(self,rt):\n",
    "        pickle.dump(self.vectorizer,open(rt+'.vec','wb'))\n",
    "        self.classifier.save(rt+'.hd5')\n",
    "        \n",
    "    def load_model_nn(self,rt):\n",
    "        self.vectorizer=pickle.load(open(rt+'.vec','rb'))\n",
    "        self.classifier=tf.keras.models.load_model(rt+'.hd5')\n",
    "    \n",
    "    def save_model(self, file):\n",
    "        pickle.dump([self.vectorizer,self.classifier],open(file,'wb'))\n",
    "                    \n",
    "    def load_model(self, file):\n",
    "        self.vectorizer,self.classifier=pickle.load(open(file,'rb'))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUIPage():\n",
    "    def __init__(self, parent, title, page_num, max_page, btn1_label=None, btn2_label=None, frontdesk=None,plumbing=None):\n",
    "        style={'description_width':'initial'}\n",
    "        self._parent_box,self.frontdesk,self.plumbing=parent,frontdesk,plumbing\n",
    "        self._title, self._btn1_label, self._btn2_label, self._page_num, self._max_page=title, \\\n",
    "            btn1_label, btn2_label, page_num, max_page\n",
    "        self._html_title=ipw.HTML(value='<h4>'+title+'</h4>')\n",
    "#        (value=title,style={'description_width':'initial','font-size':'small'},layout={'width':'800px'})\n",
    "        self.main_box=ipw.VBox([])\n",
    "        self._lb_dis, self._rb_dis=False,False\n",
    "        if page_num==0: self._lb_dis=True\n",
    "        if page_num==max_page-1: self._rb_dis=True\n",
    "        if btn1_label is None: btn1_label='Prev'\n",
    "        if btn2_label is None: btn2_label='Next'\n",
    "            \n",
    "        self._prev_btn=ipw.Button(description=btn1_label,tooltip=str(page_num),disabled=self._lb_dis,layout={'width':'200px'})\n",
    "        self._next_btn=ipw.Button(description=btn2_label,tooltip=str(page_num),disabled=self._rb_dis,layout={'width':'200px'})\n",
    "        \n",
    "        self._btm_indent_img=ipw.Image(width=1,height=50,layout={'width':'1px','height':'100px'})    \n",
    "        self._nav_box=ipw.HBox([self._prev_btn,self._next_btn])\n",
    "        self._btm_box=ipw.VBox([self._btm_indent_img,self._nav_box])\n",
    "        \n",
    "        if not frontdesk is None:\n",
    "            self.main_box.children=[frontdesk.main_box]\n",
    "    \n",
    "    def show(self):\n",
    "        self._parent_box.children=[self._html_title,self.main_box,self._btm_box]\n",
    "        if not self.frontdesk is None:\n",
    "            self.frontdesk.refresh()\n",
    "\n",
    "class GUIBook():\n",
    "    def __init__(self, pages):\n",
    "        self._num_pages=len(pages)\n",
    "        self.main_box=ipw.VBox()\n",
    "        self.pages=[]\n",
    "        for i in range(self._num_pages):\n",
    "            p=pages[i]\n",
    "            fd=p['frontdesk']\n",
    "            \n",
    "            pg=GUIPage(self.main_box,p['title'],i,self._num_pages,\n",
    "                       btn1_label=p['prev_label'],btn2_label=p['next_label'],\n",
    "                       frontdesk=fd,plumbing=p['plumbing'])\n",
    "            if fd is not None: fd.set_nav_page(pg)\n",
    "                \n",
    "            self.pages+=[pg]\n",
    "            pg._prev_btn.on_click(self._prev_click)\n",
    "            pg._next_btn.on_click(self._next_click)\n",
    "            \n",
    "        if self._num_pages>0:            \n",
    "            self._cur_page=0\n",
    "            self.pages[0].show()\n",
    "        display(self.main_box)\n",
    "        \n",
    "    def _prev_click(self,b):\n",
    "        if self._cur_page==0: return\n",
    "        self._cur_page-=1\n",
    "        self.pages[self._cur_page].show()\n",
    "    \n",
    "    def _next_click(self,b):\n",
    "        if self._cur_page==self._num_pages-1: return\n",
    "        self._cur_page+=1\n",
    "        self.pages[self._cur_page].show()          \n",
    "        \n",
    "class FrontDesk:\n",
    "    def set_nav_page(self,pg):\n",
    "        self._nav_page=pg\n",
    "        \n",
    "    def enable_nav_next(self,enable):\n",
    "        self._nav_page._next_btn.disabled=not enable\n",
    "        \n",
    "    def enable_nav_prev(self,enable):\n",
    "        self._nav_page._prev_btn.disabled=not enable\n",
    "        \n",
    "class XNATLogin(FrontDesk):\n",
    "    def __init__(self):\n",
    "        self._connected=False\n",
    "        st={'description_width':'initial'}\n",
    "        layout=ipw.Layout(margin='0 100pt 0 0')\n",
    "        layout1=ipw.Layout(justify_content='center')\n",
    "        \n",
    "        self.text1=ipw.Text(value='https://xnat-dev-mga1.nrg.wustl.edu', description='XNAT server:', \n",
    "                            layout={'width':'200pt'}, style=st, disabled=False)\n",
    "#        self.text1=ipw.Text(value='https://cnda.wustl.edu', description='XNAT server:', \n",
    "#                            layout={'width':'200pt'}, style=st, disabled=False)\n",
    "\n",
    "        self.text2=ipw.Text(value='admin',description='user:',\n",
    "                                disabled=False, style=st, layout={'width':'120pt'})\n",
    "        self.text3=ipw.Password(value='admin',description='password:',\n",
    "                                disabled=False, style=st, layout={'width':'120pt'})\n",
    "        self.lbl1=ipw.Label('status: not connected', layout={'width':'120pt'}, style=st) #layout={'width':'240px','justify-content':'center'}\n",
    "        lbl2=ipw.Label('',layout={'width':'120pt'},style=st)\n",
    "        self.btn1=ipw.Button(description=\"connect\",style={},layout={'width':'200pt'})\n",
    "        self.btn1.on_click(self.on_connect)\n",
    "        vb1=ipw.HBox([self.text1,self.text2,self.text3])\n",
    "        vb2=ipw.HBox([self.btn1,lbl2,self.lbl1])\n",
    "        self.main_box=ipw.VBox([vb1,vb2])\n",
    "        self.sp=ServerParams()        \n",
    "        \n",
    "    def refresh(self):\n",
    "        self.enable_nav_next(False)\n",
    "        \n",
    "    def on_connect(self,b):\n",
    "        #self._show_scanview(False)\n",
    "        self.lbl1.value='status: connecting...'\n",
    "        self.sp.server,self.sp.user,self.sp.password=self.text1.value,self.text2.value,self.text3.value\n",
    "        if self.sp.connect():                    \n",
    "            self.lbl1.value='status: connected'\n",
    "            self.btn1.description='Reconnect'            \n",
    "            self.connected=True\n",
    "            self.enable_nav_next(True)\n",
    "        else:\n",
    "            self.lbl1.value='status: connection failed'    \n",
    "            self.enable_nav_next(False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanClassifierLauncher(FrontDesk):\n",
    "    def __init__(self,sp):\n",
    "        self._xi=XnatIterator(sp)        \n",
    "        btn_lay={'width':'200pt'}\n",
    "        self._fupl=ipw.FileUpload(accept='.csv',multiple=False)\n",
    "        self._fupl.observe(self.read_uploaded_file)\n",
    "        self._fupl_label=ipw.Label(value='Status: waiting to upload csv',layout=btn_lay)\n",
    "        self._fupl_drop=ipw.Dropdown(options=['experiments','scans-raw','scans-classified'],\n",
    "                                     value='scans-raw',description='csv contents',disabled=False, layout=btn_lay)\n",
    "        self._fupl_box=ipw.HBox([self._fupl_drop, self._fupl, self._fupl_label])\n",
    "                \n",
    "        self._coll_btn=ipw.Button(description='Collect scans',layout=btn_lay)\n",
    "        self._coll_btn.on_click(self.collect_scans)\n",
    "        self._coll_status=ipw.Label(value='Status: ready to run.')\n",
    "        self._coll_box=ipw.HBox([self._coll_btn,self._coll_status])\n",
    "        \n",
    "        self._out_lnk=ipw.Output()        \n",
    "        \n",
    "        self._classify_btn=ipw.Button(description='Classify scans',layout=btn_lay)\n",
    "        self._classify_btn.on_click(self.run_classifier)\n",
    "        self._classify_lbl=ipw.Label(value='Status: ready')\n",
    "        self._classify_lnk=ipw.Output()\n",
    "        self._classify_box=ipw.VBox([ipw.HBox([self._classify_btn,self._classify_lbl]),self._classify_lnk])\n",
    "\n",
    "        self._group_btn=ipw.Button(description='Group scans by experiment',layout=btn_lay)\n",
    "        self._group_btn.on_click(self.group_scans)\n",
    "        self._group_lbl=ipw.Label(value='Status: ready')\n",
    "        self._group_lnk=ipw.Output()\n",
    "        self._group_box=ipw.VBox([ipw.HBox([self._group_btn,self._group_lbl]),self._group_lnk])\n",
    "\n",
    "        self._detect_gad_btn=ipw.Button(description='Detect GAD in T1hi')\n",
    "        self._detect_gad_btn.on_click(self.run_detect_gad)\n",
    "                        \n",
    "        self.main_box=ipw.VBox([self._fupl_box,self._coll_box,self._out_lnk,self._classify_box,self._group_box])\n",
    "        \n",
    "        hc=HOF_Classifier()\n",
    "        hc.load_model_nn('./scan_classifier_nn.11.26.2019')\n",
    "        self.hof_classifier=hc\n",
    "        \n",
    "        \n",
    "    def write_scans_csv(self, scans, file):\n",
    "        with open(file, 'w') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, scans[0].keys())\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(scans)\n",
    "            \n",
    "    def read_uploaded_file(self,b):\n",
    "        fupl=self._fupl\n",
    "        if not bool(fupl): return False\n",
    "        keys=list(fupl.value)        \n",
    "        try:\n",
    "            csv_reader = csv.DictReader(io.TextIOWrapper(io.BytesIO(fupl.value[keys[0]]['content'])),skipinitialspace=True)\n",
    "            self._rows=[{k: str(v) for k,v in row.items()} for row in csv_reader]\n",
    "        except:\n",
    "            self._fupl_label.value='Status: cannot parse csv'\n",
    "            return False\n",
    "        \n",
    "        self._fupl_label.value='Status: csv loaded with {} rows'.format(len(self._rows))\n",
    "        if self._fupl_drop.value=='scans-raw' or self._fupl_drop.value=='scans-classified':\n",
    "            self.scans=self._rows\n",
    "        #print(self._exps)\n",
    "        return True\n",
    "    \n",
    "    def refresh(self):\n",
    "        self.enable_nav_prev(False)\n",
    "    \n",
    "    def show_file_link(self,out,file):\n",
    "        out.outputs=();  f=FileLink(file)\n",
    "        with out:\n",
    "            display(f)\n",
    "            \n",
    "    def group_scans(self,b):\n",
    "        def add_val(dic,key,val):\n",
    "            if key in dic: dic[key]+=[val]\n",
    "            else: dic[key]=[val]\n",
    "                \n",
    "        def add_default_vals(dic,keys):\n",
    "            for key in keys:\n",
    "                if key not in dic: dic[key]=[]\n",
    "                    \n",
    "        scans=self.scans\n",
    "        d={} #experiments\n",
    "        def_keys=['T1c','T1nc','T2','T2FLAIR']\n",
    "        def_keys_imp=['T1c_imp','T1nc_imp','T2_imp','T2FLAIR_imp']\n",
    "        \n",
    "        confs={} #runtime configurations, keyed by subject. \n",
    "        for r in self.scans:\n",
    "            frames,sid,subj,exp,hofid,gad=r['frames'],r['ID'],r['subject'],r['experiment'],r['hofid'],r['gad']  \n",
    "            if exp in d: expd=d[exp]\n",
    "            else: d[exp]={}; expd=d[exp]; add_default_vals(expd,def_keys)\n",
    "            nt1c=expd['nT1c'] if 'nT1c' in expd else 0\n",
    "            expd['subject']=subj\n",
    "            if gad==\"1\" and hofid=='T1lo': add_val(expd,'T1c',sid); nt1c+=1\n",
    "            elif hofid=='MPRAGE' or hofid=='T1hi': add_val(expd,'T1nc',sid)\n",
    "            elif hofid=='T2hi': add_val(expd,'T2',sid)\n",
    "            elif hofid=='T2FLAIR': add_val(expd,'T2FLAIR',sid)\n",
    "            expd['nT1c']=nt1c\n",
    "            \n",
    "            if subj in confs: conf=confs[subj]\n",
    "            else: \n",
    "                confs[subj]={}; conf=confs[subj]; conf['experiments']=[exp]; \n",
    "                conf['subject']=subj; conf['targ_experiment']=None\n",
    "            if exp not in conf['experiments']: conf['experiments']+=[exp]\n",
    "               \n",
    "            if nt1c>0:\n",
    "                if 'nT1c' not in conf: conf['nT1c']=nt1c; conf['targ_experiment']=exp\n",
    "                elif conf['nT1c']<nt1c: conf['nT1c']=nt1c; conf['targ_experiment']=exp\n",
    "            else:\n",
    "                conf['nT1c']=0\n",
    "                           \n",
    "        #l1=list(d.values()); print(l1[:5]) \n",
    "        #l1=list(confs.values()); print(l1[:5])\n",
    "        \n",
    "        #now create run configurations, one configuration per subject.\n",
    "        for conf_key in confs.keys():\n",
    "            conf=confs[conf_key]\n",
    "            add_default_vals(conf,def_keys)\n",
    "            add_default_vals(conf,def_keys_imp)\n",
    "            texp_name=conf['targ_experiment']\n",
    "            if texp_name is not None:\n",
    "                texp=d[texp_name]\n",
    "                for k in def_keys: conf[k]=';'.join(texp[k]) if len(texp[k])>0 else 'NA'\n",
    "                for k in def_keys_imp: conf[k]='NA'\n",
    "            else: texp=None\n",
    "            for exp in conf['experiments']: \n",
    "                if exp==texp_name: continue\n",
    "                exp_dic=d[exp]\n",
    "                for i in range(len(def_keys_imp)):\n",
    "                    k,k1=def_keys[i],def_keys_imp[i]\n",
    "                    l=[ exp+\":\"+kv for kv in exp_dic[k] if len(kv)>0 ]                                        \n",
    "                    conf[k1]=';'.join(l) if len(l)>0 else 'NA'\n",
    "                    \n",
    "                '''\n",
    "                for k in def_keys:\n",
    "                    l=[ exp+\":\"+kv for kv in exp_dic[k] ]\n",
    "                    l1=[conf[k]] if len(conf[k])>0 else []\n",
    "                    l1=l1+l if len(l)>0 else l1\n",
    "                    conf[k]='' if len(l1)<1 else ','.join(l1)\n",
    "                '''\n",
    "            #print('conf:',conf)\n",
    "\n",
    "        '''                    \n",
    "        exps=[]\n",
    "        for key in d.keys():\n",
    "            e=d[key]; e['experiment']=key\n",
    "            add_default_vals(e,def_keys)\n",
    "            exps+=[e]\n",
    "            \n",
    "        '''\n",
    "        #print(exps[:20])\n",
    "        fil='run_configurations.csv'\n",
    "        vals=list(confs.values())\n",
    "        print(vals[:5])\n",
    "        self.write_scans_csv(list(confs.values()),fil)\n",
    "        self.show_file_link(self._group_lnk,fil)        \n",
    "    \n",
    "    def collect_scans(self,b):\n",
    "        subjects=[ s['Subject'] for s in self._rows ]\n",
    "        self.scans=self._xi.list_scans_all(subjects,'',self._coll_status)\n",
    "        self._coll_status='Status: found {} scans'.format(len(self.scans))\n",
    "        fil='all_scans.csv'\n",
    "        self.write_scans_csv(self.scans,fil)\n",
    "        self.show_file_link(self._out_lnk,fil)\n",
    "            \n",
    "    def run_classifier(self,b):\n",
    "        hofids=self.hof_classifier.infer_nn(self.scans)\n",
    "        for i in range(len(self.scans)):\n",
    "            self.scans[i]['hofid']=hofids[i]\n",
    "        fil='all_scans_hofid.csv'\n",
    "        self.write_scans_csv(self.scans,fil)\n",
    "        self.show_file_link(self._classify_lnk,fil)\n",
    "        \n",
    "    def run_detect_gad(self,b):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484d3145d4714d7e8b9484e62fc64386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4>Login</h4>'), VBox(children=(VBox(children=(HBox(children=(Text(value='https://…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'experiments': ['IppolitoGBMSubj006_MR'], 'subject': 'IppolitoGBMSubj006', 'targ_experiment': None, 'nT1c': 0, 'T1c': [], 'T1nc': [], 'T2': [], 'T2FLAIR': [], 'T1c_imp': 'NA', 'T1nc_imp': 'IppolitoGBMSubj006_MR:3;IppolitoGBMSubj006_MR:1', 'T2_imp': 'NA', 'T2FLAIR_imp': 'NA'}, {'experiments': ['IppolitoGBMSubj012_MR'], 'subject': 'IppolitoGBMSubj012', 'targ_experiment': None, 'nT1c': 0, 'T1c': [], 'T1nc': [], 'T2': [], 'T2FLAIR': [], 'T1c_imp': 'NA', 'T1nc_imp': 'IppolitoGBMSubj012_MR:6;IppolitoGBMSubj012_MR:1;IppolitoGBMSubj012_MR:9;IppolitoGBMSubj012_MR:10;IppolitoGBMSubj012_MR:17;IppolitoGBMSubj012_MR:18', 'T2_imp': 'IppolitoGBMSubj012_MR:11;IppolitoGBMSubj012_MR:13;IppolitoGBMSubj012_MR:19;IppolitoGBMSubj012_MR:20', 'T2FLAIR_imp': 'NA'}, {'experiments': ['IppolitoGBMSubj015_MR_part1', 'IppolitoGBMSubj015_MR_part3', 'IppolitoGBMSubj015_MR_part2'], 'subject': 'IppolitoGBMSubj015', 'targ_experiment': 'IppolitoGBMSubj015_MR_part1', 'nT1c': 1, 'T1c': '26', 'T1nc': '2', 'T2': 'NA', 'T2FLAIR': '19', 'T1c_imp': 'IppolitoGBMSubj015_MR_part2:26', 'T1nc_imp': 'NA', 'T2_imp': 'NA', 'T2FLAIR_imp': 'NA'}, {'experiments': ['IppolitoGBMSubj016_MR'], 'subject': 'IppolitoGBMSubj016', 'targ_experiment': None, 'nT1c': 0, 'T1c': [], 'T1nc': [], 'T2': [], 'T2FLAIR': [], 'T1c_imp': 'NA', 'T1nc_imp': 'IppolitoGBMSubj016_MR:2;IppolitoGBMSubj016_MR:3;IppolitoGBMSubj016_MR:37', 'T2_imp': 'IppolitoGBMSubj016_MR:29', 'T2FLAIR_imp': 'IppolitoGBMSubj016_MR:24'}, {'experiments': ['IppolitoGBMSubj017_MR_part3', 'IppolitoGBMSubj017_MR_part1', 'IppolitoGBMSubj017_MR_part2'], 'subject': 'IppolitoGBMSubj017', 'targ_experiment': None, 'nT1c': 0, 'T1c': [], 'T1nc': [], 'T2': [], 'T2FLAIR': [], 'T1c_imp': 'NA', 'T1nc_imp': 'IppolitoGBMSubj017_MR_part2:13', 'T2_imp': 'NA', 'T2FLAIR_imp': 'IppolitoGBMSubj017_MR_part2:7'}]\n"
     ]
    }
   ],
   "source": [
    "xl=XNATLogin()\n",
    "#xl.sp.project='IMIND_PILOT'\n",
    "xl.sp.project='CONDR_METS'\n",
    "\n",
    "scl=ScanClassifierLauncher(xl.sp)\n",
    "pages=[\n",
    "        {'title':'Login','frontdesk':xl,'plumbing':None,'prev_label':None,'next_label':'Configure & run'},\n",
    "        {'title':'Classify scans','frontdesk':scl,'plumbing':None,'prev_label':'Login','next_label':None}\n",
    "    ]\n",
    "g=GUIBook(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
